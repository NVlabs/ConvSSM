seed: 1
cache: false # caching available only for encoded datasets

# Training
multinode: True
batch_size: 16
eval_size: 512
num_workers: 4
lr: 0.0001
lr_schedule: "cosine"
weight_decay: 0.00001
total_steps: 500000
warmup_steps: 5000
save_interval: 2000
viz_interval: 2000
log_interval: 100

# Data
data_path: "/raid/dmlab"
eval_seq_len: 300
seq_len: 300
image_size: 64
channels: 3

num_shards: null
rng_keys: ["dropout", "sample"]
batch_keys: ["video", "actions"]

# Model
model: "teco_transformer"
vqvae_ckpt:  "./dmlab_vqgan"

encoder: # encoder / decoder are mirrored, with decoder depths reversed
  depths: [256, 512] # 16x16 -> 8x8
  blocks: 2

decoder: # encoder / decoder are mirrored, with decoder depths reversed
  depths: [256, 512] # 16x16 -> 8x8
  blocks: 4

z_ds: 8 # 8x8 -> 1x1
z_tfm_kwargs:
  embed_dim: 1024
  mlp_dim: 4096
  num_heads: 16
  num_layers: 8
  dropout: 0.
  attention_dropout: 0.

z_git:
  vocab_dim: 256
  mask_schedule: "cosine"
  tfm_kwargs:
    embed_dim: 512
    mlp_dim: 2048
    num_heads: 8
    num_layers: 8
    dropout: 0.
    attention_dropout: 0.

embedding_dim: 64
codebook:
  n_codes: 1024
  proj_dim: 32

n_cond: 1
drop_loss_rate: 0.9

# Causal Masking
causal_masking: True
frame_mask_id: -2

# Actions
use_actions: true
action_dim: 6
action_embed_dim: 16
dropout_actions: true
action_dropout_rate: 0.5
action_mask_id: -1

# Sampling
T_draft: 8
T_revise: 8
M: 2
open_loop_ctx: 36

open_loop_ctx_1: 144
action_conditioned_1: True
open_loop_ctx_2: 36
action_conditioned_2: False
